---
title: "About Me"
sitemap: false <
permalink: /
---

<link rel="stylesheet" href="../assets/style.css">
<link href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css" rel="stylesheet">

I am an incoming PhD student to Princeton Language and Intelligence (PLI) at Princeton University, advised by <a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>. My research interests broadly lie in the intersection of natural language processing and machine learning. I am currently interested in language models and agents; in particular, I aim to study the downstream effects of **pretraining data** and methods to improve the capabilities and efficiency of **reasoning** models. Below are a few questions I am interested in:

Data: 
  - How does pretraining data influence language models as sources of knowledge? <a href="https://arxiv.org/abs/2403.12958">Dated Data</a>
  - Can we attribute content generated by models back to their pretraining corpus?
  - How do we best correct misalignments arising from knowledge conflicts in models' pretraining data?
    
Reasoning:
  - Can we make reasoning models more efficient by shifting away from a discrete token space and perform reasoning in continuous latent space? <a href="https://arxiv.org/abs/2412.13171">Compressed Chain of Thought</a>
  - How much better would reasoning models be if trained with process rewards rather than just outcome rewards?
  - How can we construct environments with verifiable rewards and/or induce structure into reasoning chains to make models more capabale and efficient?

Previously, I recieved my Master's at Johns Hopkins University, advised by <a href="https://www.cs.jhu.edu/~vandurme/">Benjamin Van Durme</a>. My prior research interests were in mathematics and fluid dynamics. I performed research in these areas during my undergraduate studies at Duke University, advised by <a href="https://scholars.duke.edu/person/Tarek.Elgindi">Tarek Elgindi. 

I would love to discuss my research and any opportunities! Feel free to email me at <a href="https://www.desmos.com/calculator/0hyyj3gy3e">jeffrey.{lastname}{11*12}@gmail.com</a>.

Outside of research, I am an avid <a href="https://www.instagram.com/jeff._.climbs/">climber</a> and <a href="https://lichess.org/@/Nexx7">chess</a> player.

News
---
 
<table>
	<tr>
		<td width="15%">May 2025</td><td>Final Answer accepted to ACL 2025!</td>
  	</tr>
	<tr>
		<td width="15%">Feb 2025</td><td>New preprint, Is That Your Final Answer?, released! <a href="https://arxiv.org/abs/2502.13962">[paper]</a> <a href="https://x.com/williamjurayj/status/1892592057073512913">[tweets]</a> </td>
  	</tr>
	<tr>
		<td width="15%">Dec 2024</td><td>New preprint, Compressed Chain of Thought,  released! <a href="https://arxiv.org/abs/2412.13171">[paper]</a> <a href="https://x.com/jeff_cheng_77/status/1869474515325190299">[tweets]</a> </td>
  	</tr>
	<tr>
		<td width="15%">Oct 2024</td><td>Attended CoLM 2024 and presented Dated Data <b>(Outstanding Paper Award, 0.4% <i class="bi bi-trophy"></i>)</b></td>
	</tr>
	<tr>
		<td width="15%">Jul 2024</td><td>Dated Data accepted to CoLM 2024!</td>
	</tr>
	<tr>
		<td width="15%">Mar 2024</td><td>New preprint, Dated Data, is out! <a href="https://arxiv.org/abs/2403.12958">[paper]</a> <a href="https://x.com/jeff_cheng_77/status/1772355368649187669">[tweets]</a> </td>
	</tr>
</table>
